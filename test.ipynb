{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import paramiko\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "from flask import Request\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# SFTP Credentials\n",
        "SFTP_HOST = \"sftp.aws.directwines.com\"\n",
        "SFTP_PORT = 22\n",
        "SFTP_USER = \"aws-sftp-usecommerce\"\n",
        "SFTP_PASSWORD = os.environ.get(\"SFTP_PASSWORD\")\n",
        "SFTP_DIRECTORY = \"/\"  # Adjust if needed\n",
        "\n",
        "# Google Cloud Config\n",
        "GCS_BUCKET_NAME = \"andre_test1\"\n",
        "BQ_DATASET = \"DWI_DB\"\n",
        "BQ_TABLE = \"daily_REC008\"\n",
        "\n",
        "def get_latest_sftp_file():\n",
        "    \"\"\"Fetch the latest file from SFTP based on date in filename.\"\"\"\n",
        "    print(\"ðŸ“¢ Connecting to SFTP...\")\n",
        "    transport = paramiko.Transport((SFTP_HOST, SFTP_PORT))\n",
        "    transport.connect(username=SFTP_USER, password=SFTP_PASSWORD)\n",
        "    sftp = paramiko.SFTPClient.from_transport(transport)\n",
        "\n",
        "    print(\"ðŸ“¢ Listing available files...\")\n",
        "    files = sftp.listdir(SFTP_DIRECTORY)\n",
        "\n",
        "    # Regex pattern to match files with the expected date format\n",
        "    pattern = re.compile(r\"REC008 Website Order Type And Customer Type Report-(\\d{4}-\\d{2}-\\d{2})\")\n",
        "\n",
        "    latest_file = None\n",
        "    latest_date = None\n",
        "\n",
        "    for file in files:\n",
        "        match = pattern.search(file)\n",
        "        if match:\n",
        "            file_date = datetime.strptime(match.group(1), \"%Y-%m-%d\")\n",
        "            if latest_date is None or file_date > latest_date:\n",
        "                latest_date = file_date\n",
        "                latest_file = file\n",
        "\n",
        "    if not latest_file:\n",
        "        raise FileNotFoundError(\"ðŸš¨ No valid REC008 file found in SFTP.\")\n",
        "\n",
        "    print(f\"âœ… Latest file found: {latest_file}\")\n",
        "\n",
        "    local_file = f\"/tmp/{latest_file}\"\n",
        "    sftp.get(f\"{SFTP_DIRECTORY}/{latest_file}\", local_file)\n",
        "\n",
        "    sftp.close()\n",
        "    transport.close()\n",
        "    print(\"âœ… File downloaded from SFTP.\")\n",
        "\n",
        "    return local_file, latest_file  # Returning both local file and filename\n",
        "\n",
        "def upload_to_gcs(local_file, latest_file):\n",
        "    \"\"\"Upload the latest CSV file to Google Cloud Storage.\"\"\"\n",
        "    print(f\"ðŸ“¢ Uploading file to GCS: gs://{GCS_BUCKET_NAME}/{latest_file}\")\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "    blob = bucket.blob(latest_file)  # Keep original filename in GCS\n",
        "    blob.upload_from_filename(local_file)\n",
        "    print(\"âœ… File uploaded to GCS.\")\n",
        "\n",
        "def load_to_bigquery(latest_file):\n",
        "    \"\"\"Load CSV from GCS to BigQuery.\"\"\"\n",
        "    print(f\"ðŸ“¢ Loading CSV from GCS into BigQuery: gs://{GCS_BUCKET_NAME}/{latest_file}\")\n",
        "\n",
        "    client = bigquery.Client()\n",
        "    table_ref = client.dataset(BQ_DATASET).table(BQ_TABLE)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        autodetect=True,  # Auto-detect schema\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        field_delimiter=\",\",\n",
        "        encoding=\"UTF-8\"\n",
        "    )\n",
        "\n",
        "    uri = f\"gs://{GCS_BUCKET_NAME}/{latest_file}\"\n",
        "    load_job = client.load_table_from_uri(uri, table_ref, job_config=job_config)\n",
        "    load_job.result()  # Wait for the job to complete\n",
        "    print(\"âœ… Data loaded into BigQuery.\")\n",
        "\n",
        "def main(request: Request):\n",
        "    \"\"\"Main entry point for the Cloud Function.\"\"\"\n",
        "    print(\"ðŸš€ Cloud Function started.\")\n",
        "\n",
        "    try:\n",
        "        local_file, latest_file = get_latest_sftp_file()  # Get latest file\n",
        "        upload_to_gcs(local_file, latest_file)  # Upload to GCS\n",
        "        load_to_bigquery(latest_file)  # Load into BigQuery\n",
        "\n",
        "        print(\"âœ… Function completed successfully.\")\n",
        "        return \"Success\", 200\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸš¨ ERROR: {type(e).__name__} - {str(e)}\")\n",
        "        return f\"Error: {str(e)}\", 500"
      ],
      "metadata": {
        "id": "hymfjGIgHjvr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}